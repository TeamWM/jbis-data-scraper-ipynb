{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# 基本モジュールのインポート\n",
    "import sys, requests, datetime, lxml, os\n",
    "from itertools import repeat\n",
    "from datetime import timedelta\n",
    "\n",
    "# 日付を簡易的に変更するモジュール\n",
    "# https://qiita.com/xza/items/9618e25a8cb08c44cdb0\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ウェブスクレイピング用モジュール\n",
    "# https://qiita.com/itkr/items/513318a9b5b92bd56185\n",
    "from lxml.html import fromstring\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# 並列処理を行う上で必要なモジュール\n",
    "# https://docs.python.org/ja/3/library/multiprocessing.html\n",
    "from multiprocessing import Process\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# データフレーム格納処理に使用するモジュール\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 個人で作成したスクリプト：関数やコードのリストなど\n",
    "sys.path.insert(0, './src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  1 03:27:29 JST 2020\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 地方競馬場コードの定義\n",
    "\n",
    "データ取得先：**'https://www.jbis.or.jp/'**\n",
    "\n",
    "地方競馬場のレース結果と出場競走馬を取得するため、上記ウェブサイト上で定義されている各競馬場のコードを定義する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chihou_keibajo = {\n",
    "    \"岩見沢\": '205', \"帯広\": '206', \"旭川\": '207', \n",
    "    \"札幌\": '208', \"函館\": '209', \"門別\": '236', \n",
    "    \"水沢\": '211', \"盛岡\": '210', \"上山\": '212', \n",
    "    \"新潟\": '213', \"三条\": '214', \"足利\": '215',\n",
    "    \"宇都宮\": '216', \"高崎\": '217', \"浦和\": '218',\n",
    "    \"船橋\": '219', \"大井\": '220', \"川崎\": '221',\n",
    "    \"金沢\": '222', \"笠松\": '223', \"名古屋\": '224',\n",
    "    \"中京\": '225', \"園田\": '227', \"姫路\": '228',\n",
    "    \"益田\": '229', \"福山\": '230', \"高知\": '231',\n",
    "    \"佐賀\": '232', \"荒尾\": '233', \"中津\": '234' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 関数の定義\n",
    "\n",
    "複数回使用するコードが存在するため、それを関数とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dir(path, mkdir=True):\n",
    "    '''ディレクトリが存在するかを確認する'''\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        \n",
    "        # mkdirパラメータがtrueの場合、ディレクトリを作成する\n",
    "        if mkdir:\n",
    "            os.mkdir(path)\n",
    "            #print('{} created.'.format(path))\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    \n",
    "def get_tables(session, url):\n",
    "    '''ページソースからテーブルを取得する'''\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        soup = bs(response.content)\n",
    "        tables = soup.find_all('table')\n",
    "    except Exception as e:\n",
    "        # 現在はロガーを定義していないのでprintで返す\n",
    "        print(\"{}\".format(e))\n",
    "        return None\n",
    "    \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マルチプロセスを使用するための関数の定義\n",
    "def run(r_loc, h_loc, baseURL, url):\n",
    "    session = requests.Session()\n",
    "    tables = get_tables(session, url)\n",
    "    \n",
    "    # レース結果を取得し、保存する\n",
    "    f_name = url.split('/')[-2] + '.csv'\n",
    "    pd.read_html(str(tables))[1].to_csv(r_loc + '/' + f_name, index=False)\n",
    "    \n",
    "    # レースに出場した競走馬の情報のURLを取得する\n",
    "    h_hrefs = [baseURL + a['href'] for a in tables[1].find_all('a', href=True) if a['href'].find('horse') != -1]\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    # 各馬毎の基本情報を取得する\n",
    "    for hurl in h_hrefs:\n",
    "\n",
    "        h_desc = session.get(hurl)\n",
    "        h_soup = bs(h_desc.content)\n",
    "        h_tables = h_soup.find_all('table')\n",
    "        \n",
    "        # 競走馬情報のテーブルを作成する\n",
    "        cols = ['番号', '競走馬']\n",
    "        data = []\n",
    "        \n",
    "        # 番号をURLから取得\n",
    "        data.append(hurl.split('/')[-2])\n",
    "        \n",
    "        # 競走馬名を取得\n",
    "        data.append(h_soup.find('h1', {'class':'hdg-l1-02'}).text)\n",
    "        \n",
    "        # 競走馬の属性を取得\n",
    "        for th, td in zip(h_tables[0].find_all('th'), h_tables[0].find_all('td')):\n",
    "            cols.append(th.text)\n",
    "            data.append(td.text)\n",
    "        \n",
    "        # 競走馬の通算成績を取得(失敗した場合は、属性のみをリストに格納する)\n",
    "        try:\n",
    "            h_res = pd.read_html(str(h_tables))[2]\n",
    "            h_res.rename({'Unnamed: 0': '種類'}, axis=1, inplace=True)\n",
    "            \n",
    "            # 属性情報に結合する\n",
    "            dfs.append(pd.concat([pd.DataFrame([data], columns=cols),\n",
    "                                  pd.DataFrame([h_res[h_res['年'] == '合計'].values.ravel()])], axis=1))\n",
    "        except:\n",
    "            dfs.append(pd.DataFrame([data], columns=cols))\n",
    "            \n",
    "    # 取得したデータフレームのリストをすべて結合し、保存する\n",
    "    h_fname = url.split('/')[-4] + '_' + url.split('/')[-3] + '_' + url.split('/')[-2] + '.csv'\n",
    "    pd.concat(dfs, axis=0).to_csv(h_loc + '/' + h_fname, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. スクレイピング開始前設定\n",
    "\n",
    "    1.データ抽出開始年月の設定\n",
    "    2.データ抽出先ベースURLの設定\n",
    "    3.抽出データ格納先の設定\n",
    "    4.セッションを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20170801'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ抽出開始年月の設定(2020年6月時点での過去3年分のデータを取得)\n",
    "date_counter = datetime.date(2017, 8, 1)\n",
    "date_counter.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jbis.or.jp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ抽出先ベースURL\n",
    "baseURL = 'https://www.jbis.or.jp'\n",
    "baseURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽出データ格納先の設定\n",
    "# r_dir = レース結果格納先, h_dir = 競走馬情報の格納先\n",
    "r_dir = './data/r_res/'\n",
    "h_dir = './data/h_desc/'\n",
    "\n",
    "validate_dir(r_dir), validate_dir(h_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f946bfd2128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# セッションのインスタンス化\n",
    "session = requests.Session()\n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マルチプロセスクラスのインスタンス化\n",
    "n_proc = 24\n",
    "executor = ProcessPoolExecutor(max_workers=n_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. スクレイピング\n",
    "\n",
    "データ取得先：'https://www.jbis.or.jp/race/calendar/[YYYYMM]/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jbis.or.jp/race/calendar/201708/\n",
      "https://www.jbis.or.jp/race/calendar/201709/\n",
      "https://www.jbis.or.jp/race/calendar/201710/\n",
      "https://www.jbis.or.jp/race/calendar/201711/\n",
      "https://www.jbis.or.jp/race/calendar/201712/\n",
      "https://www.jbis.or.jp/race/calendar/201801/\n",
      "https://www.jbis.or.jp/race/calendar/201802/\n",
      "https://www.jbis.or.jp/race/calendar/201803/\n",
      "https://www.jbis.or.jp/race/calendar/201804/\n",
      "https://www.jbis.or.jp/race/calendar/201805/\n",
      "https://www.jbis.or.jp/race/calendar/201806/\n",
      "https://www.jbis.or.jp/race/calendar/201807/\n",
      "https://www.jbis.or.jp/race/calendar/201808/\n",
      "https://www.jbis.or.jp/race/calendar/201809/\n",
      "https://www.jbis.or.jp/race/calendar/201810/\n",
      "https://www.jbis.or.jp/race/calendar/201811/\n",
      "https://www.jbis.or.jp/race/calendar/201812/\n",
      "https://www.jbis.or.jp/race/calendar/201901/\n",
      "https://www.jbis.or.jp/race/calendar/201902/\n",
      "https://www.jbis.or.jp/race/calendar/201903/\n",
      "https://www.jbis.or.jp/race/calendar/201904/\n",
      "https://www.jbis.or.jp/race/calendar/201905/\n",
      "https://www.jbis.or.jp/race/calendar/201906/\n",
      "https://www.jbis.or.jp/race/calendar/201907/\n",
      "https://www.jbis.or.jp/race/calendar/201908/\n",
      "https://www.jbis.or.jp/race/calendar/201909/\n",
      "https://www.jbis.or.jp/race/calendar/201910/\n",
      "https://www.jbis.or.jp/race/calendar/201911/\n",
      "https://www.jbis.or.jp/race/calendar/201912/\n",
      "https://www.jbis.or.jp/race/calendar/202001/\n",
      "https://www.jbis.or.jp/race/calendar/202002/\n",
      "https://www.jbis.or.jp/race/calendar/202003/\n",
      "https://www.jbis.or.jp/race/calendar/202004/\n",
      "https://www.jbis.or.jp/race/calendar/202005/\n",
      "https://www.jbis.or.jp/race/calendar/202006/\n"
     ]
    }
   ],
   "source": [
    "while not date_counter.strftime('%Y%m') == '202007':\n",
    "    \n",
    "    # 月日の取得\n",
    "    ym = date_counter.strftime('%Y%m')\n",
    "    \n",
    "    # ソース取得先URLの定義(ベースURL+'/race/calendar'+月日)\n",
    "    calendarURL = baseURL + '/race/calendar/' + ym + '/'\n",
    "    \n",
    "    print(calendarURL)\n",
    "    \n",
    "    # 取得先からページソースを取得する\n",
    "    try:\n",
    "        response = session.get(calendarURL)\n",
    "        c_soup = bs(response.content)\n",
    "    except Exception as e:\n",
    "        print('{}'.format(e))\n",
    "        \n",
    "        # 取得に失敗し場合、次のループに飛ぶ\n",
    "        date_counter = date_counter + relativedelta(months=1)\n",
    "        continue\n",
    "        \n",
    "    # ページソースから日付毎に各競技場のレース結果のURLのリストを取得する\n",
    "    c_hrefs = [baseURL + a['href'] for a in c_soup.find('tbody').find_all('a', href=True)]\n",
    "    \n",
    "    # 取得したURLリストをループする\n",
    "    for href in c_hrefs:\n",
    "        \n",
    "        # テーブル取得\n",
    "        tables = get_tables(session, href)\n",
    "        \n",
    "        # 保存先ディレクトリが存在しない場合、作成する\n",
    "        r_loc_dir = r_dir + href.split('/')[-2] + '/'\n",
    "        validate_dir(r_loc_dir)\n",
    "        \n",
    "        r_loc_dir = r_loc_dir + href.split('/')[-3] + '/'\n",
    "        validate_dir(r_loc_dir)\n",
    "        \n",
    "        # pandasのread_htmlを使用してデータフレームを作成し、保存する\n",
    "        try:\n",
    "            f_name = href.split('/')[-3] + '.csv'\n",
    "            pd.read_html(str(tables))[0].to_csv(r_loc_dir + f_name, index=False, encoding='utf-8')\n",
    "    \n",
    "            # 競技場で行われた各レースごとの詳細結果があるURLを取得する\n",
    "            race_res_hrefs = [baseURL + a['href'] for a in tables[0].find_all('a', href=True) if a['href'].find('race') != -1]\n",
    "        \n",
    "            # 取得したURLごとにマルチプロセスでスクレイピングを行う\n",
    "            executor.map(run, \n",
    "                         repeat(os.path.abspath(r_loc_dir)),\n",
    "                         repeat(os.path.abspath(h_dir)),\n",
    "                         repeat(baseURL),\n",
    "                         race_res_hrefs)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #for rhref in race_res_hrefs:\n",
    "        #    tables = get_tables(session, rhref)\n",
    "        #\n",
    "        #    # レース結果を取得し、保存する\n",
    "        #    f_name = rhref.split('/')[-2] + '.csv'\n",
    "        #    pd.read_html(str(tables))[1].to_csv(r_loc_dir + f_name, index=False)\n",
    "        #    \n",
    "        #    # レースに出場した競走馬の情報のURLを取得する\n",
    "        #    h_hrefs = [a['href'] for a in tables[1].find_all('a', href=True) if a['href'].find('horse') != -1]\n",
    "        #    \n",
    "        #    dfs = []\n",
    "        #    \n",
    "        #    # 各馬毎の基本情報を取得する\n",
    "        #    for hurl in h_hrefs:\n",
    "        #\n",
    "        #        h_desc = session.get(baseURL + hurl)\n",
    "        #        h_soup = bs(h_desc.content)\n",
    "        #        h_tables = h_soup.find_all('table')\n",
    "        #        \n",
    "        #        # 競走馬情報のテーブルを作成する\n",
    "        #        cols = ['番号', '競走馬']\n",
    "        #        data = []\n",
    "        #        \n",
    "        #        # 番号をURLから取得\n",
    "        #        data.append(hurl.split('/')[-2])\n",
    "        #        \n",
    "        #        # 競走馬名を取得\n",
    "        #        data.append(h_soup.find('h1', {'class':'hdg-l1-02'}).text)\n",
    "        #        \n",
    "        #        # 競走馬の属性を取得\n",
    "        #        for th, td in zip(h_tables[0].find_all('th'), h_tables[0].find_all('td')):\n",
    "        #            cols.append(th.text)\n",
    "        #            data.append(td.text)\n",
    "        #        \n",
    "        #        # 競走馬の通算成績を取得(失敗した場合は、属性のみをリストに格納する)\n",
    "        #        try:\n",
    "        #            h_res = pd.read_html(str(h_tables))[2]\n",
    "        #            h_res.rename({'Unnamed: 0': '種類'}, axis=1, inplace=True)\n",
    "        #            \n",
    "        #            # 属性情報に結合する\n",
    "        #            dfs.append(pd.concat([pd.DataFrame([data], columns=cols),\n",
    "        #                                  pd.DataFrame([h_res[h_res['年'] == '合計'].values.ravel()])], axis=1))\n",
    "        #        except:\n",
    "        #            dfs.append(pd.DataFrame([data], columns=cols))\n",
    "        #            \n",
    "        #    # 取得したデータフレームのリストをすべて結合し、保存する\n",
    "        #    h_fname = rhref.split('/')[-4] + '_' + rhref.split('/')[-3] + '_' + rhref.split('/')[-2] + '.csv'\n",
    "        #    pd.concat(dfs, axis=0, ignore_index=True).to_csv(h_dir + h_fname, index=False, encoding='utf-8')\n",
    "    \n",
    "    # 1カ月先に進める\n",
    "    date_counter = date_counter + relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting webencodings (from html5lib)\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\n",
      "Collecting six>=1.9 (from html5lib)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: webencodings, six, html5lib\n",
      "Successfully installed html5lib-1.1 six-1.15.0 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
